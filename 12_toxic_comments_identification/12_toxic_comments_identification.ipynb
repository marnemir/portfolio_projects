{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поиск токсичных комментариев\n",
    "\n",
    "#### Тема проекта:\n",
    "- Определение токсичных комментариев пользователей интернет-магазина\n",
    "\n",
    "#### Цель:\n",
    "- Обучить модель классифицировать комментарии на позитивные и негативные (токсичные)\n",
    "\n",
    "#### Поставленные задачи:\n",
    "- Исследовать предоставленные данные;\n",
    "- Использовать несколько способов подготовки текстовых данных для обучения;\n",
    "- Обучить несколько моделей и сравнить результаты;\n",
    "- Получить значение F1-меры не ниже 0.75.\n",
    "\n",
    "#### Краткий план работы:\n",
    "- [Шаг 1. Подготовка данных](#Шаг-1.-Подготовка-данных)\n",
    "- [Шаг 2. Преобразование текстовых данных](#Шаг-2.-Преобразование-текстовых-данных)\n",
    "- [Шаг 3. Обучение моделей](#Шаг-3.-Обучение-моделей)\n",
    "  - [3.1. Логистическая регрессия (LogisticRegression)](#3.1.-Логистическая-регрессия-(LogisticRegression))\n",
    "  - [3.2. Стохастический градиентный спуск (SGDClassifier)](#3.2.-Стохастический-градиентный-спуск-(SGDClassifier))\n",
    "  - [3.3. Градиентный бустинг (LGBMClassifier)](#3.3.-Градиентный-бустинг-(LGBMClassifier))\n",
    "  \n",
    "#### Вывод:\n",
    "- Исследованы предоставленные данные;\n",
    "- Использовано три способа подготовки текстовых данных для обучения;\n",
    "- На тестовой выборке удалось достигнуть значения F1-меры не ниже 0.75.\n",
    "\n",
    "**Статус проекта**: проект завершён.\n",
    "\n",
    "**Используемые библиотеки**: *numpy*, *pandas*, *re*, *tqdm*, *sklearn*, *lightgbm*, *textblob*, *gensim*, *nltk*\n",
    "\n",
    "**Источник данных**: [курс Data Science от Яндекс.Практикум](https://praktikum.yandex.ru/profile/data-scientist/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отключение предупреждений\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# импорт библиотек и функция для дальнейшей работы\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import notebook\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\marne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# работа с текстом\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "import gensim.downloader as api\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# стоп-слова\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 1. Подготовка данных\n",
    "\n",
    "Выгрузим исходные данные, изучим общую информацию, выведем первые строки и проверим данные на наличие дубликатов.   \n",
    "Дополнительно выведем **процент токсичный комментариев** в датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество дубликатов: 0\n",
      "Доля токсичных комментариев: 10.17%\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выгрузка файла\n",
    "df = pd.read_csv('datasets/toxic_comments.csv')\n",
    "\n",
    "print('Количество дубликатов:', len(df[df.duplicated()]))\n",
    "print('Доля токсичных комментариев: {:.2%}'.format(df['toxic'].sum() / df['toxic'].count()))\n",
    "print()\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Файл содержит **159571 комментариев**; \n",
    "- **Пропусков** и **дубликатов** не обнаружено;\n",
    "- Около **10%** всех комментариев являются токсичными, т. е. наблюдается дисбаланс классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим дополнительный столбец с **лемматизированным текстом**.  \n",
    "Для этих целей используем **TextBlob Lemmatizer** с соответствующим POS-тегом.\n",
    "\n",
    "Напишем функцию, к-я будет к-я будет лемматизировать текст, выбирая для каждого слова наиболее вероятную часть речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_with_postag(sentence):   \n",
    "    \"\"\"Функция возвращает лемматизированный текст, подбирая для каждого слова часть речи:\n",
    "    - sentence - указать предложение для лемматизации.\"\"\"\n",
    "    \n",
    "    # лемматизация предложения\n",
    "    sent = TextBlob(sentence)\n",
    "    \n",
    "    # подбор части речи для нестандартных обозначений\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}\n",
    "    \n",
    "    # создание списка (слово, часть речи)\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n",
    "    \n",
    "    # список с лемматизированным текстом\n",
    "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
    "    return \" \".join(lemmatized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Like many other startup company Uber be found on the premise of disruption take an old industry oftentimes one that be a bit clunky For a short period of time company like Uber be view a economic savior They sell themselves a a mean of good use and distributing resource'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверка работы функции\n",
    "sentence = \"\"\"Like many other startup companies, Uber was founded on the premise of disruption:\n",
    "taking an old industry, oftentimes one that was a bit clunky. For a short period of time, companies like Uber were\n",
    "viewed as economic saviors. They sold themselves as a means of better using and distributing resources.\"\"\"\n",
    "\n",
    "lemmatize_with_postag(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизатор не всегда выдаёт правильную форму, но в целом он понимает:\n",
    "- Множественное число: *companies - company*\n",
    "- Герундий: *taking - take*\n",
    "- Прошедшее время: *sold - sell*\n",
    "- Глагол *be* в разных формах: *were - be*\n",
    "- Сравнительную форму прилагательных *good*, *bad*: *better* - *good*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Очистим** текст от ненужных символов и оставим только латинские буквы, использовав **шаблон**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):    \n",
    "    \"\"\"Функция убирает небуквенные символы и оставляет только слова, написанные латиницей:\n",
    "    - text - указать текст для очистки.\"\"\"   \n",
    "    cleared = re.sub(r'[^a-zA-z]', ' ', text)\n",
    "    return \" \".join(cleared.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для создания колонки с **обработанным текстом**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_df(df_name, batch_size=1000):\n",
    "    \n",
    "    \"\"\"Функция добавляет колонку с преобразованным текстом, обрабатывая исходный датафрейм по частям:\n",
    "    - df_name - наименование датафрейма;\n",
    "    - batch_size - размер батча (по умолчанию 1000 строк)\"\"\"\n",
    "    \n",
    "    # пустой финальный датафрейм\n",
    "    df_final = pd.DataFrame()\n",
    "    \n",
    "    # цикл для обработки датафрейма по частям\n",
    "    for i in notebook.tqdm(range(df_name.shape[0] // batch_size + 1)):\n",
    "        \n",
    "        df_part = df_name.iloc[batch_size*i : batch_size*(i+1)]\n",
    "            \n",
    "        # перевод всех символов в нижний регистр\n",
    "        df_part['lemm_text'] = df_part['text'].apply(str.lower)\n",
    "        \n",
    "        # очистка текста от небуквенных символов\n",
    "        df_part['lemm_text'] = df_part['lemm_text'].apply(clear_text)\n",
    "\n",
    "        # лемматизация\n",
    "        df_part['lemm_text'] = df_part['lemm_text'].apply(lemmatize_with_postag)\n",
    "        \n",
    "        df_final = df_final.append(df_part)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим новый датафрейм с лемматизированным и очищенным от посторонних символов текстом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ac3c94ccb14ffc91aab14d6baa4be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=160.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   text       159571 non-null  object\n",
      " 1   toxic      159571 non-null  int64 \n",
      " 2   lemm_text  159571 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.7+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits make under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he match this background colour i m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not try to edit war it s ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir be my hero any chance you remember wha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                           lemm_text  \n",
       "0  explanation why the edits make under my userna...  \n",
       "1  d aww he match this background colour i m seem...  \n",
       "2  hey man i m really not try to edit war it s ju...  \n",
       "3  more i can t make any real suggestion on impro...  \n",
       "4  you sir be my hero any chance you remember wha...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создание нового датафрейма\n",
    "df_lemm = lemmatize_df(df)\n",
    "\n",
    "print(df_lemm.info())\n",
    "df_lemm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительно проверим столбец *lemm_text* на наличие пропусков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество пропусков в lemm_text: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, toxic, lemm_text]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lemm_nan = df_lemm[df_lemm['lemm_text'].isnull()]\n",
    "print('Количество пропусков в lemm_text:', len(df_lemm_nan))\n",
    "df_lemm_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данных отсутствуют **пропуски**, можем приступать к следующему шагу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для дальнейших преобразований выделим **обучающую** и **тестовую** выборки в пропорции 60 на 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество элементов в обучающей выборке: 95743\n",
      "Количество элементов в тестовой выборке: 63828\n"
     ]
    }
   ],
   "source": [
    "SEED = 18\n",
    "\n",
    "# выделение обучающей и тестовой выборки\n",
    "df_train = df_lemm.sample(frac=0.6, random_state=SEED)\n",
    "df_test = df_lemm[~df_lemm.index.isin(df_train.index)]\n",
    "\n",
    "# выделение признаков и целевого признака\n",
    "X_train = df_train['lemm_text']\n",
    "y_train = df_train['toxic']\n",
    "X_test = df_test['lemm_text']\n",
    "y_test = df_test['toxic']\n",
    "\n",
    "print('Количество элементов в обучающей выборке:', X_train.shape[0])\n",
    "print('Количество элементов в тестовой выборке:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 1. Вывод\n",
    "\n",
    "#### Изучен файл:\n",
    "- Файл содержит **159571 комментариев**, из к-х **10%** являются **токсичными**.\n",
    "- **Пропусков** и **дубликатов** не обнаружено.\n",
    "    \n",
    "#### Проведена лемматизация текста:\n",
    "- Леммы выделены с помощью **TextBlob Lemmatizer**.\n",
    "- Из-за большого объема исходный датафрейм был преобразован **по частям (батчам)**.\n",
    "- Выделены **обучающая** и **тестовая** выборки в пропорции 60:40."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 2. Преобразование текстовых данных\n",
    "\n",
    "Для подготовки текстовых данных для дальнейшего обучения воспользуемся **несколькими техниками**:\n",
    "- [2.1. Метод TF-IDF](#2.1.-Метод-TF-IDF)\n",
    "- [2.2. Использование Word2Vec](#2.2.-Использование-Word2Vec)\n",
    "- [2.3. Использование сторонних корпусов](#2.3.-Использование-сторонних-корпусов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Метод TF-IDF\n",
    "\n",
    "Проведём сентимент-анализ, использовав расчёт величины **TF-IDF** для каждого текста.\n",
    "\n",
    "Для обучения необходимо привести **признаки** в соответствующий вид. Разделим выборку на **обучающую** и **тестовую** и для каждой из выборок преобразуем признаки, использовав `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы train: (95743, 117443)\n",
      "Размер матрицы test: (63828, 117443)\n"
     ]
    }
   ],
   "source": [
    "# создание TfidfVectorizer\n",
    "count_tf_idf = TfidfVectorizer(stop_words = stopwords)\n",
    "\n",
    "# выборка train\n",
    "corpus_train = X_train.values.astype('U')\n",
    "X_train_tfidf = count_tf_idf.fit_transform(corpus_train)\n",
    "print(\"Размер матрицы train:\", X_train_tfidf.shape)\n",
    "\n",
    "# выборка test\n",
    "corpus_test = X_test.values.astype('U')\n",
    "X_test_tfidf = count_tf_idf.transform(corpus_test)\n",
    "print(\"Размер матрицы test:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы перевели **признаки** в необходимый для обучения вид."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Использование Word2Vec\n",
    "\n",
    "Представим наши слова в виде **векторов** с помощью метода построения языковых представлений **word2vec**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    \"\"\"Итератор, который выдает обработанное предложение.\"\"\"    \n",
    "    def __iter__(self):                \n",
    "        for line in X_train.values: # обучающая выборка\n",
    "            # в строке одна запись, разделенная пробелами\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание образца класса\n",
    "sentences = MyCorpus()\n",
    "w2v = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее близкие слова:\n",
      "[('debate', 0.7768642902374268), ('rfc', 0.621604859828949), ('conversation', 0.6086463332176208), ('thread', 0.6054226160049438), ('afd', 0.5890777707099915), ('comment', 0.5802565217018127), ('consensus', 0.5626112222671509), ('ani', 0.5513968467712402), ('discuss', 0.5490802526473999), ('dispute', 0.5428581237792969)]\n",
      "\n",
      "Векторное представление:\n",
      "[-3.0868955   0.42404908  2.7263534  -0.8208147  -2.148401    2.8722901\n",
      "  1.2545977  -0.41676378  3.121291    1.4188262  -1.0123551  -0.4688601\n",
      " -2.9280999  -1.3276205   2.7306988  -2.4798932   3.6329334  -2.4391897\n",
      " -0.7627519  -0.27007627  1.5489968  -0.23699255 -0.44199198  0.37721848\n",
      "  0.20171142  3.4347224  -2.8656614  -1.3479848  -0.80399007  0.32610008\n",
      " -1.3552976  -0.58394593  0.08817018 -0.16374959 -1.0531813   0.48285526\n",
      "  0.2503096  -0.38590214 -1.8812141  -0.3798565   0.01637984 -1.9802777\n",
      " -3.2029028  -1.4443629   0.16734827 -0.49736717  2.03025    -3.0164073\n",
      "  0.4867485  -2.9834347   1.7389363   0.1163712  -0.4804828   0.01102999\n",
      " -0.7314167   2.8978004  -0.19903103 -0.41553098 -0.87422884 -1.3832384\n",
      " -1.877257    1.0533762   3.3776338   0.744547   -0.85248506  1.8762132\n",
      " -1.0990647   0.4509443   0.3271165  -0.03539397  0.99466884 -0.4672159\n",
      " -2.330522   -1.9691705  -0.75425476  2.4723694   1.064758   -1.2540654\n",
      " -1.5179782  -0.75449115  2.0192924  -0.44027582 -0.4347434  -0.02153442\n",
      " -0.37142578  0.46179998  1.4300314  -2.2324893   1.0413473  -1.8193841\n",
      " -0.71596855 -0.9684877   1.0190002   0.3160356   0.30416757 -0.07875137\n",
      "  2.1090074   0.05884914  0.19874722 -1.0528752 ]\n"
     ]
    }
   ],
   "source": [
    "# проверка работы\n",
    "print('Наиболее близкие слова:')\n",
    "print(w2v.most_similar('discussion'))\n",
    "print()\n",
    "\n",
    "print('Векторное представление:')\n",
    "print(w2v['discussion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения суммарных **векторов** по всей строке напишем функцию, которая будет принимать на вход строку и складывать в ней все вектора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(row):\n",
    "    \"\"\"Функция принимает на вход строку и выдает сумму векторов.\"\"\"   \n",
    "    vecs = [np.zeros(100)] # вектор заполняется нулями\n",
    "    \n",
    "    for word in utils.simple_preprocess(row):\n",
    "        try:\n",
    "            vector = w2v[word]\n",
    "        except:\n",
    "            vector = np.zeros(100) # если новое слово, чтобы модель не выдала ошибку, сделаем заполнение нулями\n",
    "        vecs.append(vector)\n",
    "        \n",
    "    return np.sum(np.array(vecs), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы train: (95743, 100)\n",
      "Размер матрицы test: (63828, 100)\n"
     ]
    }
   ],
   "source": [
    "# перевод \n",
    "X_train_vw = X_train.apply(get_vectors)\n",
    "X_test_vw = X_test.apply(get_vectors)\n",
    "\n",
    "X_train_vw = pd.DataFrame(X_train_vw.to_list())\n",
    "X_test_vw = pd.DataFrame(X_test_vw.to_list())\n",
    "\n",
    "print(\"Размер матрицы train:\", X_train_vw.shape)\n",
    "print(\"Размер матрицы test:\", X_test_vw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторные значения:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-19.143094</td>\n",
       "      <td>6.001502</td>\n",
       "      <td>4.879802</td>\n",
       "      <td>-20.931625</td>\n",
       "      <td>-17.077759</td>\n",
       "      <td>22.206276</td>\n",
       "      <td>5.614954</td>\n",
       "      <td>-15.418019</td>\n",
       "      <td>20.530026</td>\n",
       "      <td>14.200071</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.811645</td>\n",
       "      <td>-28.734429</td>\n",
       "      <td>8.731041</td>\n",
       "      <td>21.691564</td>\n",
       "      <td>8.653783</td>\n",
       "      <td>10.486366</td>\n",
       "      <td>-6.300555</td>\n",
       "      <td>2.425375</td>\n",
       "      <td>7.174889</td>\n",
       "      <td>14.669803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-16.377851</td>\n",
       "      <td>9.115148</td>\n",
       "      <td>1.233155</td>\n",
       "      <td>-29.787657</td>\n",
       "      <td>-25.885999</td>\n",
       "      <td>16.485670</td>\n",
       "      <td>8.186777</td>\n",
       "      <td>-16.994646</td>\n",
       "      <td>47.848925</td>\n",
       "      <td>20.773394</td>\n",
       "      <td>...</td>\n",
       "      <td>3.052235</td>\n",
       "      <td>-42.483007</td>\n",
       "      <td>24.919971</td>\n",
       "      <td>26.989490</td>\n",
       "      <td>-12.588672</td>\n",
       "      <td>7.161145</td>\n",
       "      <td>3.161629</td>\n",
       "      <td>-2.554231</td>\n",
       "      <td>1.208275</td>\n",
       "      <td>2.393457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.549082</td>\n",
       "      <td>4.965007</td>\n",
       "      <td>-1.214038</td>\n",
       "      <td>-5.276573</td>\n",
       "      <td>-14.215235</td>\n",
       "      <td>2.852126</td>\n",
       "      <td>1.038043</td>\n",
       "      <td>-6.409001</td>\n",
       "      <td>8.126548</td>\n",
       "      <td>4.270383</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.514260</td>\n",
       "      <td>-7.959220</td>\n",
       "      <td>5.352983</td>\n",
       "      <td>4.553497</td>\n",
       "      <td>-6.374660</td>\n",
       "      <td>2.872086</td>\n",
       "      <td>-4.318406</td>\n",
       "      <td>-3.123333</td>\n",
       "      <td>-2.142425</td>\n",
       "      <td>1.448300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.825354</td>\n",
       "      <td>5.732467</td>\n",
       "      <td>8.188472</td>\n",
       "      <td>-13.569016</td>\n",
       "      <td>-22.181368</td>\n",
       "      <td>14.738446</td>\n",
       "      <td>8.062176</td>\n",
       "      <td>-10.192916</td>\n",
       "      <td>16.839786</td>\n",
       "      <td>10.515175</td>\n",
       "      <td>...</td>\n",
       "      <td>6.216455</td>\n",
       "      <td>-13.937288</td>\n",
       "      <td>3.456885</td>\n",
       "      <td>16.229399</td>\n",
       "      <td>5.464751</td>\n",
       "      <td>3.663197</td>\n",
       "      <td>-3.650932</td>\n",
       "      <td>2.354220</td>\n",
       "      <td>-4.489151</td>\n",
       "      <td>-1.423013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-47.559546</td>\n",
       "      <td>10.653450</td>\n",
       "      <td>-25.181754</td>\n",
       "      <td>-36.298261</td>\n",
       "      <td>-66.837556</td>\n",
       "      <td>33.488656</td>\n",
       "      <td>9.884977</td>\n",
       "      <td>-17.041061</td>\n",
       "      <td>65.342199</td>\n",
       "      <td>33.004103</td>\n",
       "      <td>...</td>\n",
       "      <td>8.437258</td>\n",
       "      <td>-78.356642</td>\n",
       "      <td>32.521686</td>\n",
       "      <td>57.653056</td>\n",
       "      <td>-11.413193</td>\n",
       "      <td>1.911562</td>\n",
       "      <td>-19.040217</td>\n",
       "      <td>-2.502533</td>\n",
       "      <td>-38.604903</td>\n",
       "      <td>7.508386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0          1          2          3          4          5         6   \\\n",
       "0 -19.143094   6.001502   4.879802 -20.931625 -17.077759  22.206276  5.614954   \n",
       "1 -16.377851   9.115148   1.233155 -29.787657 -25.885999  16.485670  8.186777   \n",
       "2   2.549082   4.965007  -1.214038  -5.276573 -14.215235   2.852126  1.038043   \n",
       "3  -5.825354   5.732467   8.188472 -13.569016 -22.181368  14.738446  8.062176   \n",
       "4 -47.559546  10.653450 -25.181754 -36.298261 -66.837556  33.488656  9.884977   \n",
       "\n",
       "          7          8          9   ...        90         91         92  \\\n",
       "0 -15.418019  20.530026  14.200071  ... -1.811645 -28.734429   8.731041   \n",
       "1 -16.994646  47.848925  20.773394  ...  3.052235 -42.483007  24.919971   \n",
       "2  -6.409001   8.126548   4.270383  ... -3.514260  -7.959220   5.352983   \n",
       "3 -10.192916  16.839786  10.515175  ...  6.216455 -13.937288   3.456885   \n",
       "4 -17.041061  65.342199  33.004103  ...  8.437258 -78.356642  32.521686   \n",
       "\n",
       "          93         94         95         96        97         98         99  \n",
       "0  21.691564   8.653783  10.486366  -6.300555  2.425375   7.174889  14.669803  \n",
       "1  26.989490 -12.588672   7.161145   3.161629 -2.554231   1.208275   2.393457  \n",
       "2   4.553497  -6.374660   2.872086  -4.318406 -3.123333  -2.142425   1.448300  \n",
       "3  16.229399   5.464751   3.663197  -3.650932  2.354220  -4.489151  -1.423013  \n",
       "4  57.653056 -11.413193   1.911562 -19.040217 -2.502533 -38.604903   7.508386  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вывод нескольких строк\n",
    "print('Векторные значения:')\n",
    "X_test_vw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Использование сторонних корпусов\n",
    "\n",
    "Для переведения текстовых данных в вектора мы можем использовать не только наши непосредственные данные, но и брать сторонние корпуса. В данном случае проведем обучение **на данных из твиттера**.\n",
    "\n",
    "Источник данных: https://github.com/RaRe-Technologies/gensim-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка модели из glove-twitter-50\n",
    "gt50 = api.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее близкие слова:\n",
      "[('article', 0.842178225517273), ('conversation', 0.7992870211601257), ('dialogue', 0.7992134094238281), ('discuss', 0.7935640215873718), ('discussions', 0.7880508899688721), ('summary', 0.7877680063247681), ('questions', 0.7793878316879272), ('articles', 0.7715831398963928), ('regarding', 0.7698727250099182), ('content', 0.7649972438812256)]\n",
      "\n",
      "Векторное представление:\n",
      "[ 1.457     0.34705  -0.070119  0.23195   1.5397    0.47888   0.57762\n",
      " -0.6027   -0.053207 -0.623    -0.39628  -0.71816  -2.5486    0.063757\n",
      " -0.11111   0.3769    0.3596    0.38589  -0.26696   0.19303  -0.36622\n",
      " -0.65895   0.038764 -0.3426   -0.4924    0.27062   0.29772  -0.060013\n",
      " -0.88368   0.65449   0.20583  -0.36151  -0.64415  -0.56266   0.41122\n",
      " -0.078038 -0.028325  0.45015   1.2699   -0.60587  -0.021684 -0.069277\n",
      "  1.5698    0.35513   0.31453   0.22604  -0.21975   0.11178  -0.071925\n",
      " -0.60919 ]\n"
     ]
    }
   ],
   "source": [
    "# проверка работы\n",
    "print('Наиболее близкие слова:')\n",
    "print(gt50.most_similar('discussion'))\n",
    "print()\n",
    "\n",
    "print('Векторное представление:')\n",
    "print(gt50['discussion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_gt50(row):\n",
    "    \"\"\"Функция принимает на вход строку и выдает сумму векторов.\"\"\"\n",
    "    vecs = [np.zeros(50)]\n",
    "    \n",
    "    for word in utils.simple_preprocess(row):\n",
    "        try:\n",
    "            vector = gt50[word]\n",
    "        except:\n",
    "            vector = np.zeros(50)\n",
    "        vecs.append(vector)\n",
    "    return np.sum(np.array(vecs), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы train: (95743, 50)\n",
      "Размер матрицы test: (63828, 50)\n"
     ]
    }
   ],
   "source": [
    "# перевод \n",
    "X_train_gt50 = X_train.apply(get_vectors_gt50)\n",
    "X_test_gt50 = X_test.apply(get_vectors_gt50)\n",
    "\n",
    "X_train_gt50 = pd.DataFrame(X_train_gt50.to_list())\n",
    "X_test_gt50 = pd.DataFrame(X_test_gt50.to_list())\n",
    "\n",
    "print(\"Размер матрицы train:\", X_train_gt50.shape)\n",
    "print(\"Размер матрицы test:\", X_test_gt50.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторные значения:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.368033</td>\n",
       "      <td>9.440956</td>\n",
       "      <td>-2.382837</td>\n",
       "      <td>-2.361844</td>\n",
       "      <td>4.640430</td>\n",
       "      <td>-5.951034</td>\n",
       "      <td>29.204004</td>\n",
       "      <td>2.189049</td>\n",
       "      <td>4.824542</td>\n",
       "      <td>-3.292482</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.780592</td>\n",
       "      <td>0.327232</td>\n",
       "      <td>4.489789</td>\n",
       "      <td>-0.203634</td>\n",
       "      <td>5.328064</td>\n",
       "      <td>-5.635204</td>\n",
       "      <td>0.529677</td>\n",
       "      <td>-1.843277</td>\n",
       "      <td>-2.686622</td>\n",
       "      <td>-2.414388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.144589</td>\n",
       "      <td>11.760614</td>\n",
       "      <td>-1.919324</td>\n",
       "      <td>-1.420990</td>\n",
       "      <td>2.929482</td>\n",
       "      <td>6.436321</td>\n",
       "      <td>31.874813</td>\n",
       "      <td>-2.906626</td>\n",
       "      <td>-0.839530</td>\n",
       "      <td>1.256420</td>\n",
       "      <td>...</td>\n",
       "      <td>-31.090605</td>\n",
       "      <td>7.391714</td>\n",
       "      <td>8.837459</td>\n",
       "      <td>-1.773951</td>\n",
       "      <td>6.971064</td>\n",
       "      <td>-9.509069</td>\n",
       "      <td>4.811804</td>\n",
       "      <td>1.997747</td>\n",
       "      <td>-8.962230</td>\n",
       "      <td>-2.732993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.060950</td>\n",
       "      <td>3.844096</td>\n",
       "      <td>0.097410</td>\n",
       "      <td>-0.037453</td>\n",
       "      <td>0.959599</td>\n",
       "      <td>1.422480</td>\n",
       "      <td>4.797704</td>\n",
       "      <td>0.316074</td>\n",
       "      <td>-0.663709</td>\n",
       "      <td>-1.244771</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.148280</td>\n",
       "      <td>0.118720</td>\n",
       "      <td>1.647180</td>\n",
       "      <td>1.704293</td>\n",
       "      <td>1.073768</td>\n",
       "      <td>-3.173065</td>\n",
       "      <td>0.571897</td>\n",
       "      <td>1.586039</td>\n",
       "      <td>-0.344900</td>\n",
       "      <td>0.784278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.636494</td>\n",
       "      <td>5.187800</td>\n",
       "      <td>-2.288114</td>\n",
       "      <td>-2.367339</td>\n",
       "      <td>-0.120624</td>\n",
       "      <td>-1.119048</td>\n",
       "      <td>13.630130</td>\n",
       "      <td>2.635803</td>\n",
       "      <td>1.327785</td>\n",
       "      <td>2.038338</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.583170</td>\n",
       "      <td>-0.745174</td>\n",
       "      <td>5.736978</td>\n",
       "      <td>1.469561</td>\n",
       "      <td>4.692824</td>\n",
       "      <td>-5.189890</td>\n",
       "      <td>-1.372413</td>\n",
       "      <td>-0.900442</td>\n",
       "      <td>-0.511644</td>\n",
       "      <td>-3.045695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.469137</td>\n",
       "      <td>17.929430</td>\n",
       "      <td>-2.010879</td>\n",
       "      <td>0.394288</td>\n",
       "      <td>10.650006</td>\n",
       "      <td>7.511726</td>\n",
       "      <td>56.685291</td>\n",
       "      <td>-7.435640</td>\n",
       "      <td>4.314865</td>\n",
       "      <td>4.987215</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.977134</td>\n",
       "      <td>2.679926</td>\n",
       "      <td>20.633760</td>\n",
       "      <td>-5.916283</td>\n",
       "      <td>15.626647</td>\n",
       "      <td>-17.553925</td>\n",
       "      <td>7.748345</td>\n",
       "      <td>5.928867</td>\n",
       "      <td>-9.601481</td>\n",
       "      <td>-0.511386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0          1         2         3          4         5          6   \\\n",
       "0   1.368033   9.440956 -2.382837 -2.361844   4.640430 -5.951034  29.204004   \n",
       "1  12.144589  11.760614 -1.919324 -1.420990   2.929482  6.436321  31.874813   \n",
       "2   3.060950   3.844096  0.097410 -0.037453   0.959599  1.422480   4.797704   \n",
       "3   1.636494   5.187800 -2.288114 -2.367339  -0.120624 -1.119048  13.630130   \n",
       "4  17.469137  17.929430 -2.010879  0.394288  10.650006  7.511726  56.685291   \n",
       "\n",
       "         7         8         9   ...         40        41         42  \\\n",
       "0  2.189049  4.824542 -3.292482  ... -22.780592  0.327232   4.489789   \n",
       "1 -2.906626 -0.839530  1.256420  ... -31.090605  7.391714   8.837459   \n",
       "2  0.316074 -0.663709 -1.244771  ...  -7.148280  0.118720   1.647180   \n",
       "3  2.635803  1.327785  2.038338  ... -12.583170 -0.745174   5.736978   \n",
       "4 -7.435640  4.314865  4.987215  ... -43.977134  2.679926  20.633760   \n",
       "\n",
       "         43         44         45        46        47        48        49  \n",
       "0 -0.203634   5.328064  -5.635204  0.529677 -1.843277 -2.686622 -2.414388  \n",
       "1 -1.773951   6.971064  -9.509069  4.811804  1.997747 -8.962230 -2.732993  \n",
       "2  1.704293   1.073768  -3.173065  0.571897  1.586039 -0.344900  0.784278  \n",
       "3  1.469561   4.692824  -5.189890 -1.372413 -0.900442 -0.511644 -3.045695  \n",
       "4 -5.916283  15.626647 -17.553925  7.748345  5.928867 -9.601481 -0.511386  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вывод нескольких строк\n",
    "print('Векторные значения:')\n",
    "X_test_gt50.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 2. Вывод\n",
    "\n",
    "Для подготовки текстовых данных для дальнейшего обучения было использовано несколько техник:\n",
    "- **Метод TF-IDF**. Проведена оценка важности и частоты слова во встречающемся тексте.\n",
    "- **Использование Word2Vec**. Произведен расчёт векторов для каждой строки.\n",
    "- **Использование сторонних корпусов**. Был использован открытый корпус *glove-twitter-50* для векторизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 3. Обучение моделей\n",
    "\n",
    "Рассмотрим, какие результаты получатся в случае обучения на выборках, подготовленных с помощью разных техник векторизации.  \n",
    "В качестве моделей выберем следующие:\n",
    "\n",
    "- [3.1. Логистическая регрессия (LogisticRegression)](#3.1.-Логистическая-регрессия-(LogisticRegression))\n",
    "- [3.2. Стохастический градиентный спуск (SGDClassifier)](#3.2.-Стохастический-градиентный-спуск-(SGDClassifier))\n",
    "- [3.3. Градиентный бустинг (LGBMClassifier)](#3.3.-Градиентный-бустинг-(LGBMClassifier))\n",
    "\n",
    "В данном случае мы **не будем подбирать гиперпараметры** для моделей, а проверим все выборке на одной заранее выбранной комбинации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Логистическая регрессия (LogisticRegression)\n",
    "\n",
    "Напишем функцию, которая будет проводить проверку на тестовой выборке и возвращать **матрицу с метриками**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model(X_train_name, X_test_name, method_name=None):\n",
    "    \"\"\"Обучение с помощью логистической регрессии для указанных выборок.\"\"\"\n",
    "    # обучение модели\n",
    "    lr = LogisticRegression(solver='liblinear', penalty='l2', class_weight='balanced', C=15, random_state=SEED)\n",
    "    lr.fit(X_train_name, y_train)\n",
    "    y_pred = lr.predict(X_test_name)\n",
    "    \n",
    "    print(f\"Значение f1 ({method_name}): {f1_score(y_test, y_pred)}\")\n",
    "    print()\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение f1 (метод TF-IDF): 0.7515021459227469\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     57311\n",
      "           1       0.70      0.81      0.75      6517\n",
      "\n",
      "    accuracy                           0.95     63828\n",
      "   macro avg       0.84      0.88      0.86     63828\n",
      "weighted avg       0.95      0.95      0.95     63828\n",
      "\n",
      "Wall time: 3.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# метод TF-IDF\n",
    "lr_model(X_train_tfidf, X_test_tfidf, 'метод TF-IDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- На тестовой выборке значение *f1* немного **выше** необходимого **0.75**.\n",
    "- Обучение происходит очень **быстро** - всего за 3-4 секунды.\n",
    "- **Precision** на уровне **0.7**, значит, из 100 определенных моделью комментариев как \"токсичные\" реально такими является 70 комментариев.\n",
    "- **Recall** на уровне **0.81**, значит, верно определяется 81% токсичных комментариев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение f1 (метод Word2Vec): 0.5648409282074117\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.92     57311\n",
      "           1       0.42      0.88      0.56      6517\n",
      "\n",
      "    accuracy                           0.86     63828\n",
      "   macro avg       0.70      0.87      0.74     63828\n",
      "weighted avg       0.93      0.86      0.88     63828\n",
      "\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# метод Word2Vec\n",
    "lr_model(X_train_vw, X_test_vw, 'метод Word2Vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Результаты *f1* получились **ниже** необходимого значения **0.75**.\n",
    "- **Recall** выше, чем в предыдущем случае, т. е. верно определяется 88% токсичных комментариев.\n",
    "- Однако гораздо ниже **precision**, что говорит о высокой доле ложноположительных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение f1 (glove-twitter-50): 0.5484558568383483\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.92     57311\n",
      "           1       0.41      0.85      0.55      6517\n",
      "\n",
      "    accuracy                           0.86     63828\n",
      "   macro avg       0.69      0.85      0.73     63828\n",
      "weighted avg       0.92      0.86      0.88     63828\n",
      "\n",
      "Wall time: 7.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# glove-twitter-50\n",
    "lr_model(X_train_gt50, X_test_gt50, 'glove-twitter-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Снова результат по *f1* не дотягивает до 0.75.\n",
    "- Аналогичная проблема, что и в предыдущем случае - неплохое значение по **recall**, однако достаточно низкий показатель по **precision**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Стохастический градиентный спуск (SGDClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_model(X_train_name, X_test_name, method_name=None):\n",
    "    \"\"\"Обучение с помощью стохастического градиентного спуска для указанных выборок.\"\"\"\n",
    "    # обучение модели\n",
    "    sgd = SGDClassifier(loss='modified_huber', penalty='l2', class_weight='balanced', alpha=0.00001, random_state=SEED)\n",
    "    sgd.fit(X_train_name, y_train)\n",
    "    y_pred = sgd.predict(X_test_name)\n",
    "    \n",
    "    print(f\"Значение f1 ({method_name}): {f1_score(y_test, y_pred)}\")\n",
    "    print()\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение f1 (метод TF-IDF): 0.7315934065934067\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97     57311\n",
      "           1       0.66      0.82      0.73      6517\n",
      "\n",
      "    accuracy                           0.94     63828\n",
      "   macro avg       0.82      0.88      0.85     63828\n",
      "weighted avg       0.95      0.94      0.94     63828\n",
      "\n",
      "Wall time: 356 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# метод TF-IDF\n",
    "sgd_model(X_train_tfidf, X_test_tfidf, 'метод TF-IDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Результат *f1* также получился немного **ниже** требуемого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение f1 (метод Word2Vec): 0.31584812948000307\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.53      0.69     57311\n",
      "           1       0.19      0.96      0.32      6517\n",
      "\n",
      "    accuracy                           0.58     63828\n",
      "   macro avg       0.59      0.75      0.50     63828\n",
      "weighted avg       0.91      0.58      0.66     63828\n",
      "\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# метод Word2Vec\n",
    "sgd_model(X_train_vw, X_test_vw, 'метод Word2Vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Как и в случае с логистической регрессией, результат *f1* получился **ниже** необходимого.\n",
    "- Также наблюдается сильный разрыв между значениями **precision** и **recall**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение f1 (glove-twitter-50): 0.6303480533773559\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95     57311\n",
      "           1       0.57      0.70      0.63      6517\n",
      "\n",
      "    accuracy                           0.92     63828\n",
      "   macro avg       0.77      0.82      0.79     63828\n",
      "weighted avg       0.93      0.92      0.92     63828\n",
      "\n",
      "Wall time: 1.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# glove-twitter-50\n",
    "sgd_model(X_train_gt50, X_test_gt50, 'glove-twitter-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Для данной модели значение *f1* стало **выше**, однако вё равно недостаточно хорошим.\n",
    "- Значительно выросо значение по **precision**, однако при этом гораздо хуже стал **recall**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Градиентный бустинг (LGBMClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_model(X_train_name, X_test_name, method_name=None):\n",
    "    \"\"\"Обучение с помощью градиентного бустинга для указанных выборок.\"\"\"\n",
    "    # обучение модели\n",
    "    lgbm = LGBMClassifier(n_estimators=150, learning_rate=0.3, random_state=SEED)\n",
    "    lgbm.fit(X_train_name, y_train)\n",
    "    y_pred = lgbm.predict(X_test_name)\n",
    "    \n",
    "    print(f\"Значение f1 ({method_name}): {f1_score(y_test, y_pred)}\")\n",
    "    print()\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение f1 (метод TF-IDF): 0.7671068427370948\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     57311\n",
      "           1       0.87      0.69      0.77      6517\n",
      "\n",
      "    accuracy                           0.96     63828\n",
      "   macro avg       0.92      0.84      0.87     63828\n",
      "weighted avg       0.96      0.96      0.96     63828\n",
      "\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# метод TF-IDF\n",
    "lgbm_model(X_train_tfidf, X_test_tfidf, 'метод TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение f1 (метод Word2Vec): 0.64152919612464\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96     57311\n",
      "           1       0.74      0.56      0.64      6517\n",
      "\n",
      "    accuracy                           0.94     63828\n",
      "   macro avg       0.85      0.77      0.80     63828\n",
      "weighted avg       0.93      0.94      0.93     63828\n",
      "\n",
      "Wall time: 2.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# метод Word2Vec\n",
    "lgbm_model(X_train_vw, X_test_vw, 'метод Word2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение f1 (glove-twitter-50): 0.6169056738220361\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96     57311\n",
      "           1       0.74      0.53      0.62      6517\n",
      "\n",
      "    accuracy                           0.93     63828\n",
      "   macro avg       0.84      0.75      0.79     63828\n",
      "weighted avg       0.93      0.93      0.93     63828\n",
      "\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# glove-twitter-50\n",
    "lgbm_model(X_train_gt50, X_test_gt50, 'glove-twitter-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Приемлемый результат получился только на выборке с *методом TF-IDF* - результат *f1* около **0.77**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверенных моделей сделаем сводную таблицу с результатами:\n",
    "\n",
    "| Тип выборки | Способ | Результат f1 |\n",
    "|:------------|:-------|-------------:|\n",
    "|Метод TF-IDF|Логистическая регрессия |0.7515|\n",
    "|Метод Word2Vec|Логистическая регрессия|0.5648|\n",
    "|Метод glove-twitter-50|Логистическая регрессия|0.5485|\n",
    "|Метод TF-IDF|Стохастический градиентный спуск |0.7316|\n",
    "|Метод Word2Vec|Стохастический градиентный спуск|0.3158|\n",
    "|Метод glove-twitter-50|Стохастический градиентный спуск|0.6303|\n",
    "|Метод TF-IDF|Градиентный бустинг |0.7671|\n",
    "|Метод Word2Vec|Градиентный бустинг |0.6415|\n",
    "|Метод glove-twitter-50|Градиентный бустинг|0.6169|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 3. Вывод\n",
    "\n",
    "- Для большинства типов выборок наилучшие результаты получены с помощью **градиентного бустинга**.\n",
    "- В нашем случае наилучшие результаты получались на выборке **TF-IDF**.\n",
    "- В случае **TF-IDF** на тестовой выборке получены показатели *f1* **выше 0.75**, за исключением стохастического градиентного спуска.\n",
    "- В остальных случаях показатели **не дотягивали** до нужного значения.\n",
    "\n",
    "Стоит сделать важное замечание, что мы **не подбирали** оптимальную комбинацию гиперпараметров для каждой модели, а использовали только один вариант. Возможно, при подборе наилучших гиперпараметров мы смогли бы **улушить** результаты *f1* на других типах выборок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итоговый вывод\n",
    "\n",
    "#### Подготовка\n",
    "- Полученный файл содержал **159571 комментариев**, из к-х **10%** являлись **токсичными**.\n",
    "- С помощью **TextBlob Lemmatizer** проведена **лемматизация**.\n",
    "- Из-за большого массива данных обработка текста была проведена **по частям (батчам)**.\n",
    "- Выделены **обучающая** и **тестовая** выборки в пропорции 60:40.\n",
    "\n",
    "Для подготовки текстовых данных для дальнейшего обучения было использовано несколько техник:\n",
    "- **Метод TF-IDF**. Проведена оценка важности и частоты слова во встречающемся тексте.\n",
    "- **Использование Word2Vec**. Произведен расчёт векторов для каждой строки.\n",
    "- **Использование сторонних корпусов**. Был использован открытый корпус *glove-twitter-50* для векторизации.\n",
    "\n",
    "#### Финальные результаты\n",
    "| Тип выборки | Способ | Результат f1 |\n",
    "|:------------|:-------|-------------:|\n",
    "|Метод TF-IDF|Логистическая регрессия |0.7515|\n",
    "|Метод Word2Vec|Логистическая регрессия|0.5648|\n",
    "|Метод glove-twitter-50|Логистическая регрессия|0.5485|\n",
    "|Метод TF-IDF|Стохастический градиентный спуск |0.7316|\n",
    "|Метод Word2Vec|Стохастический градиентный спуск|0.3158|\n",
    "|Метод glove-twitter-50|Стохастический градиентный спуск|0.6303|\n",
    "|Метод TF-IDF|Градиентный бустинг |0.7671|\n",
    "|Метод Word2Vec|Градиентный бустинг |0.6415|\n",
    "|Метод glove-twitter-50|Градиентный бустинг|0.6169|\n",
    "\n",
    "- Для большинства типов выборок наилучшие результаты получены с помощью **градиентного бустинга**.\n",
    "- В нашем случае наилучшие результаты получались на выборке **TF-IDF**.\n",
    "- В случае **TF-IDF** на тестовой выборке получены показатели *f1* **выше 0.75**, за исключением стохастического градиентного спуска.\n",
    "- В остальных случаях показатели **не дотягивали** до нужного значения.\n",
    "\n",
    "Стоит сделать важное замечание, что мы **не подбирали** оптимальную комбинацию гиперпараметров для каждой модели, а использовали только один вариант. Возможно, при подборе наилучших гиперпараметров мы смогли бы **улушить** результаты *f1* на других типах выборок."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
